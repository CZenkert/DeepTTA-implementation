{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bc2add4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import MWETokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "import timeit\n",
        "import scipy"
      ],
      "id": "7bc2add4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7zsMCUCl9IA",
        "outputId": "81c16b9e-a9c4-49ae-99a3-888b41a7a4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "z7zsMCUCl9IA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHbHG47VpMuO",
        "outputId": "40405f48-348b-4688-fe30-edecf87f42ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['drug_codes_chembl_freq_1500.txt', 'SMILEinchi.csv', 'GDSC2_fitted_dose_response_25Feb20.xlsx', 'Cell_line_RMA_proc_basalExp.txt', 'token.pkl', 'model_checkpoint', 'Drugs2Vec.ipynb']\n"
          ]
        }
      ],
      "source": [
        "# The files 'drug_codes_chembl_freq_1500.txt', 'SMILEinchi.csv', 'GDSC2_fitted_dose_response_25Feb20.xlsx', 'Cell_line_RMA_proc_basalExp.txt'\n",
        "# need to be uploaded to google drive in data_path for the notebook to work.\n",
        "data_path = '/content/drive/MyDrive/DeepTTA/data'\n",
        "files = os.listdir(data_path)\n",
        "print(files)\n"
      ],
      "id": "DHbHG47VpMuO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "87d5863c",
        "outputId": "072451d6-df1a-4b3d-fc42-7b7990c69fa2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    1635  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1049  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    150   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1502  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1512  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    172   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    179   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1073  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    288   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1578  ...  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  ...   \n",
              "\n",
              "                                                    2107  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1908  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    2047  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1916  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1873  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    17    \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    153   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    190   \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1034  \\\n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
              "\n",
              "                                                    1911  \n",
              "token  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
              "\n",
              "[1 rows x 354 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1fbd67d6-fbb8-4908-bf4c-df4730ebe4c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1635</th>\n",
              "      <th>1049</th>\n",
              "      <th>150</th>\n",
              "      <th>1502</th>\n",
              "      <th>1512</th>\n",
              "      <th>172</th>\n",
              "      <th>179</th>\n",
              "      <th>1073</th>\n",
              "      <th>288</th>\n",
              "      <th>1578</th>\n",
              "      <th>...</th>\n",
              "      <th>2107</th>\n",
              "      <th>1908</th>\n",
              "      <th>2047</th>\n",
              "      <th>1916</th>\n",
              "      <th>1873</th>\n",
              "      <th>17</th>\n",
              "      <th>153</th>\n",
              "      <th>190</th>\n",
              "      <th>1034</th>\n",
              "      <th>1911</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>token</th>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows Ã— 354 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fbd67d6-fbb8-4908-bf4c-df4730ebe4c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1fbd67d6-fbb8-4908-bf4c-df4730ebe4c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1fbd67d6-fbb8-4908-bf4c-df4730ebe4c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "class Tokenization(MWETokenizer):\n",
        "\n",
        "    def __init__(self):\n",
        "        MWETokenizer.__init__(self)\n",
        "        vocab_file=open(data_path +'/drug_codes_chembl_freq_1500.txt','r')\n",
        "        vocab_data=vocab_file.read()\n",
        "        vocab_data=vocab_data.replace(' ','')\n",
        "        self.vocab=vocab_data.split('\\n')\n",
        "        self.vocab.pop(0)\n",
        "        self.max_length= len(max(self.vocab, key=len))\n",
        "        self.zeta=58 # Max lengt of drug representation\n",
        "\n",
        "        SMILES_CHARS = [' ','#', '%', '(', ')', '+', '-', '.', '/','0', '1', '2', '3', '4', '5', '6', '7', '8', '9','=',\n",
        "                        '@','A', 'B', 'C', 'F', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P','R', 'S', 'T', 'V', 'X', 'Z',\n",
        "                        '[', '\\\\', ']','a', 'b', 'c', 'e', 'g', 'i', 'l', 'n', 'o', 'p', 'r', 's','t', 'u']\n",
        "        self.vocab=self.vocab+SMILES_CHARS\n",
        "\n",
        "        self.l=len(self.vocab)\n",
        "\n",
        "        self.smi2index = dict( (c,i) for i,c in enumerate( self.vocab ) )\n",
        "        self.index2smi = dict( (i,c) for i,c in enumerate( self.vocab ) )\n",
        "\n",
        "        #NLTK:\n",
        "        self.nltk_vocab=[]\n",
        "        for token in self.vocab:\n",
        "            self.nltk_vocab.append(tuple([*token]))\n",
        "        self.NLTK_tokenizer = MWETokenizer(self.nltk_vocab,separator='')\n",
        "\n",
        "    # tokenization function using NLTK package:\n",
        "    def smiles_to_token(self,smiles):\n",
        "        tokenized_lst=self.NLTK_tokenizer.tokenize([*smiles])\n",
        "        X = np.zeros( (self.l, self.zeta ) )\n",
        "        for j in range(len(tokenized_lst)):\n",
        "            i=self.smi2index[tokenized_lst[j]]\n",
        "            X[i,j]=1\n",
        "        return X\n",
        "\n",
        "    def token_to_smiles(self,X ):\n",
        "        smi = ''\n",
        "        X = X.argmax( axis=0 )\n",
        "        for i in X:\n",
        "            if(i==0): break\n",
        "            smi += index2smi[ i ]\n",
        "        return smi\n",
        "\n",
        "    # Function to tokenize all drugs in the SMILES file and save them in a token.pkl file in data_path. This file is needed in the custom dataset.\n",
        "    # Needs to be run once to create this file.\n",
        "    def tokenize_file(self):\n",
        "        drug_data=pd.read_csv(data_path + '/SMILEinchi.csv')\n",
        "        smiles_lst=list(drug_data['smiles'])\n",
        "        drugID_lst=list(drug_data['drug_id'])\n",
        "        token_lst=[]\n",
        "        for entry in smiles_lst:\n",
        "            token_lst.append(self.smiles_to_token(entry))\n",
        "        output=pd.DataFrame(list(zip(drugID_lst,token_lst)),columns=['drugID','token'])\n",
        "        output=output.T\n",
        "        output=output.drop(['drugID'])\n",
        "        output=output.set_axis(drugID_lst,axis=1)\n",
        "        output= output.loc[:,~output.columns.duplicated()].copy()\n",
        "        output.to_pickle(data_path +'/token.pkl')\n",
        "        return output\n",
        "\n",
        "tok=Tokenization()\n",
        "tok.tokenize_file()\n"
      ],
      "id": "87d5863c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f835086"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self,l,zeta,gamma):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.l=l # Input dim of embedding/lenght of tokenized substructure vector. Nedds to match output of Tokenization\n",
        "        self.zeta=zeta # Max lengt of drug representation. Needs to match with output of Tokenization\n",
        "        self.gamma=gamma # Output dim of embedding. Needs to match input of Transformer layer and be divisible by 8\n",
        "\n",
        "        dropout_rate=0.05\n",
        "\n",
        "        # Layers:\n",
        "        self.chem_embedding=torch.nn.Linear(self.l,self.gamma,bias=False)\n",
        "        self.pos_embedding=torch.nn.Linear(self.zeta,self.gamma,bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self,M):\n",
        "        for i in range(M.size(0)):\n",
        "            C=torch.zeros([self.gamma,self.zeta])\n",
        "            P=torch.zeros([self.gamma,self.zeta])\n",
        "            for j in range(self.zeta):\n",
        "                C[:,j]=self.chem_embedding(M[i,:,j])\n",
        "                I=torch.zeros(self.zeta)\n",
        "                I=I.to(M.device)\n",
        "                I[j]=1\n",
        "                P[:,j]=self.pos_embedding(I)\n",
        "            E_batch=torch.add(C,P)\n",
        "            if i==0:\n",
        "                E=E_batch.unsqueeze(0)\n",
        "            else:\n",
        "                E_batch=E_batch.unsqueeze(0)\n",
        "                E=torch.cat((E,E_batch),0)\n",
        "        E=self.dropout(E)\n",
        "        return (E.to(M.device))\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,gamma):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=gamma, nhead=8, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "    def forward(self,E):\n",
        "        output=self.transformer_encoder(E.permute(0,2,1))\n",
        "        return output"
      ],
      "id": "4f835086"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFQnx6ETSyHC"
      },
      "outputs": [],
      "source": [
        "class CombinedDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,gene_pca=False):\n",
        "\n",
        "        # loading files\n",
        "        self.gene_data=pd.read_table(data_path + '/Cell_line_RMA_proc_basalExp.txt')\n",
        "        GDSC_data=pd.read_excel(data_path + '/GDSC2_fitted_dose_response_25Feb20.xlsx')\n",
        "        self.drug_data=pd.read_pickle(data_path +'/token.pkl')\n",
        "\n",
        "        # reduce size of gene_data file\n",
        "        self.gene_names=list(self.gene_data.columns)\n",
        "        self.gene_names.pop(0)\n",
        "        self.gene_names.pop(0)\n",
        "        self.gene_data=self.gene_data.T\n",
        "        self.gene_data=self.gene_data.drop(['GENE_SYMBOLS','GENE_title'])\n",
        "        self.gene_data=self.gene_data.astype('float32')\n",
        "\n",
        "        if gene_pca:\n",
        "            pca = PCA(n_components=1018)\n",
        "            pca.fit(self.gene_data)\n",
        "            self.gene_data=pca.transform(self.gene_data)\n",
        "\n",
        "        self.gene_data=self.gene_data.T\n",
        "        self.gene_data=pd.DataFrame(self.gene_data)\n",
        "        self.gene_data=self.gene_data.set_axis(self.gene_names,axis=1)\n",
        "\n",
        "        COSMIC_lst0= list(GDSC_data['COSMIC_ID'])\n",
        "        drugID_lst0= list(GDSC_data['DRUG_ID'])\n",
        "        IC50_lst0= list(GDSC_data['LN_IC50'])\n",
        "        self.COSMIC_lst=[]\n",
        "        self.drugID_lst=[]\n",
        "        self.IC50_lst=[]\n",
        "\n",
        "        # dropping entries with no corresponding gene/drug data\n",
        "        n_fails=0\n",
        "        for i in range(len(COSMIC_lst0)):\n",
        "            try:\n",
        "                self.gene_data['DATA.'+str(COSMIC_lst0[i])]\n",
        "                self.drug_data[drugID_lst0[i]]\n",
        "\n",
        "                self.COSMIC_lst.append(COSMIC_lst0[i])\n",
        "                self.drugID_lst.append(drugID_lst0[i])\n",
        "                self.IC50_lst.append(IC50_lst0[i])\n",
        "            except: n_fails+=1\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.IC50_lst))\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        gene_expression=torch.Tensor(list(self.gene_data['DATA.'+str(self.COSMIC_lst[i])]))\n",
        "        drug_token=torch.Tensor(self.drug_data.loc['token',self.drugID_lst[i]])\n",
        "        IC50_value=torch.Tensor([self.IC50_lst[i]])\n",
        "        return gene_expression, drug_token, IC50_value"
      ],
      "id": "FFQnx6ETSyHC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0c743bf",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# gene_pca reduces rna input dim from 17737 to 1018, somewhat faster. Layers in the model have to be adjusted if changed.\n",
        "dat=CombinedDataset(gene_pca=True)"
      ],
      "id": "d0c743bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0cff337"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.epoch=0\n",
        "        self.device='cpu'\n",
        "        self.emb=Embedding(2713,58,8)\n",
        "        self.trafo=Transformer(8)\n",
        "        self.dropout=nn.Dropout()\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.lin_1=nn.Linear(1018,256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.lin_2=nn.Linear(256,64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.class_1=nn.Linear(528,64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.class_2=nn.Linear(64,8)\n",
        "        self.bn4 = nn.BatchNorm1d(8)\n",
        "        self.class_3=nn.Linear(8,1)\n",
        "        self.optimizer=torch.optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    def forward(self,rna,drug):\n",
        "        # rna linear layers\n",
        "        rna=self.lin_1(rna)\n",
        "          #rna=self.bn1(rna)\n",
        "          #rna=self.relu(rna)\n",
        "        rna=self.lin_2(rna)\n",
        "          #rna=self.bn2(rna)\n",
        "          #rna=self.relu(rna)\n",
        "        # drug embedding and transformer layers\n",
        "        drug=self.emb(drug)\n",
        "        drug=self.trafo(drug)\n",
        "        drug=torch.flatten(drug,1)\n",
        "        # classifier layer\n",
        "        x=torch.cat((rna,drug),dim=1)\n",
        "        x=self.class_1(x)\n",
        "          #x=self.bn3(x)\n",
        "          #x=self.relu(x)\n",
        "        x=self.class_2(x)\n",
        "          #x=self.bn4(x)\n",
        "          #x=self.relu(x)\n",
        "        output=self.class_3(x)\n",
        "        return output\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'epoch': self.epoch,\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict()\n",
        "            }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        checkpoint = torch.load(path,map_location=torch.device('cpu'))\n",
        "        self.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.epoch=checkpoint['epoch']\n",
        "\n",
        "\n",
        "net=Model()"
      ],
      "id": "d0cff337"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GTSYisvoR5U"
      },
      "outputs": [],
      "source": [
        "# load model weights from file, optional:\n",
        "net.load(data_path +'/model1_checkpoint')"
      ],
      "id": "3GTSYisvoR5U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXgt1nbSwRaA",
        "outputId": "4863e1ce-554e-4f17-9fcf-7f3f136721e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Moving the model and the dataset to GPU, if available\n",
        "def to_gpu(model):\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device('cuda')  # GPU device\n",
        "  else:\n",
        "      device = torch.device('cpu')   # CPU device\n",
        "  model.to(device)\n",
        "  model.device=device\n",
        "  print('device:', device)\n",
        "\n",
        "  # Moving loaded optimizer parameters to GPU\n",
        "  for state in model.optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(device)\n",
        "\n",
        "to_gpu(net)\n",
        "\n",
        "# Function to move tensors in each batch of the dataloader to the GPU\n",
        "def custom_collate(batch):\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device('cuda')\n",
        "    else:\n",
        "      device = torch.device('cpu')\n",
        "    gene, drug, target = zip(*batch)\n",
        "    gene = torch.stack(gene).to(device)\n",
        "    drug = torch.stack(drug).to(device)\n",
        "    target = torch.stack(target).to(device)\n",
        "    return gene, drug, target\n",
        "\n",
        "# split dataset and define dataloader\n",
        "train_set, test_set=torch.utils.data.random_split(dat, [int(0.9*dat.__len__()), dat.__len__()-int(0.9*dat.__len__())])\n",
        "\n",
        "trainloader=torch.utils.data.DataLoader(train_set,batch_size=128,shuffle=True, collate_fn=custom_collate)\n",
        "testloader=torch.utils.data.DataLoader(test_set,batch_size=128, collate_fn=custom_collate)"
      ],
      "id": "iXgt1nbSwRaA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "178a71fb",
        "outputId": "3caddcf6-eb31-46f9-afd4-1894894c1c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: 0 train loss: 5.905\n",
            "batch: 100 train loss: 7.875\n",
            "batch: 200 train loss: 8.732\n",
            "batch: 300 train loss: 6.814\n",
            "batch: 400 train loss: 8.409\n",
            "batch: 500 train loss: 6.162\n",
            "batch: 600 train loss: 8.323\n",
            "batch: 700 train loss: 7.774\n",
            "epochs: 2 test loss: 6.96336 correlation: 0.3357 time: 50 min\n",
            "batch: 0 train loss: 5.441\n",
            "batch: 100 train loss: 7.617\n",
            "batch: 200 train loss: 7.432\n",
            "batch: 300 train loss: 6.278\n",
            "batch: 400 train loss: 5.073\n",
            "batch: 500 train loss: 6.488\n",
            "batch: 600 train loss: 6.544\n",
            "batch: 700 train loss: 8.695\n",
            "epochs: 3 test loss: 6.90235 correlation: 0.3366 time: 100 min\n"
          ]
        }
      ],
      "source": [
        "# Training function. Running will overwrite existing model weights file!\n",
        "def train(model,dataloader,n_epochs):\n",
        "    start_time = timeit.default_timer()\n",
        "    loss_lst=[]\n",
        "    optimizer = model.optimizer\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, threshold=0.01, verbose=True)\n",
        "    if model.epoch==0:\n",
        "      epoch_loss, correlation=test(model,testloader)\n",
        "      loss_lst.append(epoch_loss)\n",
        "      time = timeit.default_timer()\n",
        "      print('epochs:', model.epoch,'test loss:',round(epoch_loss,5),'correlation:', round(correlation,4),'time:',int((time-start_time)/60),'min')\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "      model.train()\n",
        "\n",
        "      for batch_id, (rna,drug,target) in enumerate(dataloader):\n",
        "          optimizer.zero_grad()\n",
        "          output=model(rna,drug)\n",
        "          loss=torch.nn.functional.mse_loss(output,target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_id%100 ==0: print('batch:',batch_id, 'train loss:',round(loss.item(),3))\n",
        "      model.epoch+=1\n",
        "      epoch_loss, correlation=test(model,testloader)\n",
        "      scheduler.step(epoch_loss)\n",
        "      # creating checkpoints by saving model weights once per epoch:\n",
        "      model.save(data_path +'/model1_checkpoint')\n",
        "      loss_lst.append(epoch_loss)\n",
        "      time = timeit.default_timer()\n",
        "      print('epochs:', model.epoch,'test loss:',round(epoch_loss,5),'correlation:', round(correlation,4),'time:',int((time-start_time)/60),'min')\n",
        "\n",
        "# test function to evaluate model performance on test set\n",
        "def test(model,dataloader):\n",
        "  model.eval()\n",
        "  avg_loss=0\n",
        "  avg_correlation=0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_id, (rna,drug,target) in enumerate(dataloader):\n",
        "      output=model(rna,drug)\n",
        "      loss=torch.nn.functional.mse_loss(output,target).item()\n",
        "      correlation=scipy.stats.pearsonr(np.squeeze(output.cpu().numpy()), np.squeeze(target.cpu().numpy()))[0]\n",
        "      avg_loss=(loss+batch_id*avg_loss)/(batch_id+1)\n",
        "      avg_correlation=(correlation+batch_id*avg_correlation)/(batch_id+1)\n",
        "      #if batch_id==10: break\n",
        "  return avg_loss, avg_correlation\n",
        "\n",
        "\n",
        "train(net,trainloader,2)\n",
        "\n"
      ],
      "id": "178a71fb"
    },
    {
      "cell_type": "code",
      "source": [
        "print(net.optimizer.lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuqXMXfb-s8e",
        "outputId": "ca6246f5-c422-440e-c360-5bb58bfaf0a5"
      },
      "id": "QuqXMXfb-s8e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vjvrdGVZtPE",
        "outputId": "a7aabebe-8656-4d66-dfc8-3936681360b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 545513\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in net.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ],
      "id": "7vjvrdGVZtPE"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}